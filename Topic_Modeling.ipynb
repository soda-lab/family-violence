{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sqlite3\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtz = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    if word.startswith('#'):\n",
    "        return word\n",
    "    \n",
    "    lemma = lmtz.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lmtz.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def strip_punc(s):\n",
    "    if s[0] == '#' or s[0] == '@':\n",
    "        return s\n",
    "    return ''.join([c for c in s if c.isalpha()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [strip_punc(w) for w in stopwords.words('english')]\n",
    "stop_words.extend([\n",
    "    'i',\n",
    "    'u',\n",
    "    'r',\n",
    "    'im',\n",
    "    'cant',\n",
    "    'would',\n",
    "    'family',\n",
    "    'domestic',\n",
    "    'violence',\n",
    "    'australia',\n",
    "    'australian',\n",
    "    'dv',\n",
    "    'fv',\n",
    "    'wa',\n",
    "    'via',\n",
    "    'today',\n",
    "    'thing',\n",
    "    'make',\n",
    "    'talk',\n",
    "    'due',\n",
    "    'day',\n",
    "    'month',\n",
    "    'find',\n",
    "    'show',\n",
    "    'put',\n",
    "    'part',\n",
    "    'time',\n",
    "    'yeah',\n",
    "    'deal',\n",
    "    'big',\n",
    "    'level',\n",
    "    'focus',\n",
    "    'theyre',\n",
    "    'list',\n",
    "    'top',\n",
    "    'give',\n",
    "    'situation',\n",
    "    'lot',\n",
    "    'hold',\n",
    "    'number',\n",
    "    'include',\n",
    "    'form',\n",
    "    'back',\n",
    "    'involve',\n",
    "    'link',\n",
    "    'real'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('dpc.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "year = 2018\n",
    "\n",
    "data = []\n",
    "replied_ids = []\n",
    "for (date, text, ftext, replied_id) in c.execute('SELECT created_at, text, [extended_tweet.full_text], in_reply_to_status_id FROM tweets'):\n",
    "    if date.split(' ')[-1] == str(year):\n",
    "        if ftext:\n",
    "            data.append(ftext)\n",
    "        else:\n",
    "            data.append(text)\n",
    "        \n",
    "        if replied_id:\n",
    "            replied_ids.append(replied_id)\n",
    "        else:\n",
    "            replied_ids.append(None)\n",
    "        \n",
    "conn.close()\n",
    "print(len(data))\n",
    "len(replied_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cleaned_sent = set()\n",
    "data_lemmatized = []\n",
    "data_filtered = []\n",
    "indices = [] # points back to index in original data\n",
    "\n",
    "for i, sent in enumerate(data):\n",
    "    \n",
    "    cleaned_sent = ''\n",
    "    for token in sent.split():\n",
    "        \n",
    "        # Cleaning\n",
    "        if token[0] in ['@','$','%','^','&','*'] or token.startswith('http'):\n",
    "            continue\n",
    "\n",
    "        # Remove puctuations, lower case\n",
    "        token = strip_punc(token.lower())\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemma = lemmatize(token)\n",
    "\n",
    "        if lemma and lemma not in stop_words:\n",
    "            cleaned_sent += lemma + ' '\n",
    "    \n",
    "    cleaned_sent = cleaned_sent.strip()\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if cleaned_sent and cleaned_sent not in set_cleaned_sent:\n",
    "        set_cleaned_sent.add(cleaned_sent)\n",
    "        data_lemmatized.append(cleaned_sent.split())\n",
    "        data_filtered.append(sent)\n",
    "        indices.append(i)\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(data_lemmatized[:50]):\n",
    "    print(indices[i], sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_lemmatized, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_lemmatized], threshold=100)  \n",
    "\n",
    "# Faster way to get a Tweet clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = make_bigrams(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LDA Mallet Model\n",
    "Malletâ€™s version of LDA often gives a better quality of topics.\n",
    "Num of topics = 20 at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "mallet_path = 'mallet-2.0.8/bin/mallet' # update this path\n",
    "# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=12, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal number of topics for LDA\n",
    "Build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        print(str(len(model_list)-1)+'-'+str(num_topics),end=' ')\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic coherence provide a convenient measure to judge how good a given topic model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time to run.\n",
    "start = 9\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=start, limit=start+1, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_list[0]\n",
    "\n",
    "#from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most representative document for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the dominant topic in each Tweet\n",
    "\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus=corpus, texts=data_filtered):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=data_filtered)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute dominant topic for each Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 40 Tweets under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_dominant_topic.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(10)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Format\n",
    "# sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "len(sent_topics_sorteddf_mallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014\n",
    "#topic_names = ['Action needed','Legal action',\"Men's actions\",'Advocacy & campaigning','Government action','Violence justified','Service','Abuse experiences','Social causes','Culture of Violence']\n",
    "# 2015\n",
    "#topic_names = ['Culture & attitudes','Government action','Services needed',\"Men's actions\",'Advocacy & campaigning','Policing','Prevalence']\n",
    "#2016\n",
    "#topic_names = ['Violence & policing','Male perpetrators','Government inaction','Community support','Law reform','Successful programs','Social determinants',\"Victims' Experiences\"]\n",
    "#2017\n",
    "topic_names = ['Social context','Male perpetrators','Survival and Inspiration','Prevalence & risk','Programs & services','Men as victims']\n",
    "#2018\n",
    "#topic_names = ['Gun violence','Prevalence',\"Men's actions\",'Contexts & causes','Prevention Strategy','Government inaction','Abuse experiences','Advocacy & Campaigning','Politics & Governance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Key','Doc_Id'])\n",
    "\n",
    "for i, row in sent_topics_sorteddf_mallet.iterrows():\n",
    "        topic_i = int(row['Dominant_Topic'])\n",
    "        df.loc[i] = [str(year)+topic_names[topic_i], indices[row['Document_No']]]\n",
    "\n",
    "df.to_csv('output/topic_tweets/{}_topic_tweets.csv'.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(i1, i2):\n",
    "    vectors = model.get_topics()\n",
    "    return 1 - cosine(vectors[i1], vectors[i2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/topic_sim/2018.csv', 'w') as f:\n",
    "    f.write('Topic,And_Topic,Similarity\\n')\n",
    "    for i1 in range(start-1):\n",
    "        for i2 in range(i1+1, start):\n",
    "            f.write('{},{},{}\\n'.format(i1+1, i2+1, get_sim(i1,i2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [15,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "for i1 in range(start-1):\n",
    "    for i2 in range(i1+1, start):\n",
    "        G.add_edge(i1, i2, weight=get_sim(i1,i2)*100)\n",
    "\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "labels = {}\n",
    "for i in range(start):\n",
    "    labels[i] = str(i)\n",
    "\n",
    "color = '#E74C3C'\n",
    "nx.draw_networkx_nodes(G,pos,node_color=color,node_size=2000)\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=16)\n",
    "nx.draw(G, pos, edges=edges, width=weights, node_color=color, edge_color='grey')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('2017')\n",
    "plt.savefig(\"output/topic_sim/2017.png\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sim(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=21; start=4; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.figure().suptitle('2018')\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the 3 optimal models\n",
    "optimal_model = model_list[0]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show\n",
    "x=0\n",
    "for text in df_dominant_topic[df_dominant_topic['Dominant_Topic']==5.0]['Text']:\n",
    "    print(text, '\\n')\n",
    "    x+=1\n",
    "    if x>30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 2 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "counts2 = {}\n",
    "for text in sent_topics_sorteddf_mallet[(sent_topics_sorteddf_mallet['Dominant_Topic']==2.0) | (sent_topics_sorteddf_mallet['Dominant_Topic']==5.0)]['Text']:\n",
    "    for token in text.strip().split():\n",
    "        if token.startswith('#'):\n",
    "            tag = strip_punc(token).lower()\n",
    "            counts[tag] = counts.get(tag,0)+1\n",
    "        if token.startswith('@'):\n",
    "            tag = strip_punc(token).lower()\n",
    "            counts2[tag] = counts2.get(tag,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for tag in list(reversed(sorted(counts, key=counts.get)))[:30]:\n",
    "    print(tag+':', counts[tag])\n",
    "print()\n",
    "for tag in list(reversed(sorted(counts2, key=counts2.get)))[:30]:\n",
    "    print(tag+':', counts2[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top handlers for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'abcnews':'ABC News',\n",
    "    'whiteribbonaust':'White Ribbon',\n",
    "    'qanda': 'ABC Q&A',\n",
    "    'thetodayshow':'The Today Show',\n",
    "    'theage':'The Age',\n",
    "    'acurrentaffair':'A Current Affair (marketplace)',\n",
    "    'theprojecttv':'The Project (news)',\n",
    "    'dvvic':'DV Vic',\n",
    "    'smh':'Sydney Morning Herald',\n",
    "    'dv':'Demostic Violence',\n",
    "    'buzzrothfield':'Phil Rothfield (sports journalist)',\n",
    "    'tonyabbottmhr': 'Tony Abbott',\n",
    "    'charliepick':'Charlie Pickering (host)',\n",
    "    'danielandrewsmp': 'Dan Andrews (MP)',\n",
    "    'rosiebatty': 'Rosie Batty',\n",
    "    'billshortenmp': 'Bill Shorten',\n",
    "    'corybernardi':'Cory Bernardi (conservative)',\n",
    "    'mariska': 'Mariska Hargitay (actress)',\n",
    "    'mikebairdmp': 'Mike Baird (politician)',\n",
    "    'colleenhartland': 'Colleen Hartland (MP)',\n",
    "    'changeaus': 'Change.org',\n",
    "    'dailylifeau': 'Daily Life',\n",
    "    'conversationedu': 'The Conversation (news analysis)',\n",
    "    'familycourtau':'Family Court',\n",
    "    'womensagenda':\"Women's Agenda (news)\",\n",
    "    'turnbullmalcolm': 'Malcolm Turnbull',\n",
    "    'annastaciamp': 'Annastacia Palaszczuk (MP)',\n",
    "    'johnmcaldwell': 'John Caldwell (reporter)',\n",
    "    'lizbroderick':'Elizabeth Broderick (activist)',\n",
    "    'frichardsonmp': 'Fiona Richardson (MP)',\n",
    "    'erin': 'Erin Caton (activist)',\n",
    "    'vanbadham': 'Van Badham (commentator)',\n",
    "    'kate': 'Kate (artist)',\n",
    "    'carringtonkl': 'Kerry Carrington (professor)',\n",
    "    'skinnergj':\"Dougy's Daily Digest\",\n",
    "    'newscomauhq':'news.com.au',\n",
    "    'youtube': 'Youtube',\n",
    "    'bairdjulia':'Dr Julia Baird (journalist)',\n",
    "    'paulinehansonoz':'Pauline Hanson (politician)',\n",
    "    'lyleshelton':'Lyle Shelton (lobbyist)',\n",
    "    'mirandadevine':'Miranda Devine (columnist)',\n",
    "    'senatorcash':'Michaelia Cash (minister)',\n",
    "    'skynewsaust':'Sky News',\n",
    "    'nrl':'National Rugby League',\n",
    "    'ourwatchaus':'Our Watch',\n",
    "    'realmarklatham':'Mark Latham (politician)',\n",
    "    'nathutchins':'Natalie Hutchins (politician)',\n",
    "    'peterdutton':'Peter Dutton (MP)',\n",
    "    'perkinsmiki':'Miki Perkins (editor)',\n",
    "    'canberratimes':'Canberra Times',\n",
    "    'dancingceo': 'Dancing CEO (lifestyle)',\n",
    "    'scottmorrisonmp': 'Scott Morrison',\n",
    "    'hpu': \"Hawaii Pacific Univ.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler_counts_each_topic = [{},{},{},{},{},{},{},{}]\n",
    "for _, row in sent_topics_sorteddf_mallet.iterrows():\n",
    "    topic_idx = int(row['Dominant_Topic'])\n",
    "    dic = handler_counts_each_topic[topic_idx]\n",
    "    text = data[indices[int(row['Document_No'])]]\n",
    "\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('@'):\n",
    "            token += '/'\n",
    "            i = 1\n",
    "            while token[i].isalpha():\n",
    "                i += 1\n",
    "            if i > 1:\n",
    "                handler = token[0:i].lower()\n",
    "                dic[handler] = dic.get(handler, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(handler_counts_each_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influencers_df = pd.read_csv('middle-data/influencers-5.csv')\n",
    "influencers_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "influencers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "x = 2016\n",
    "table.append(['topic_idx','influencer_idx','count','topic','influencer'])\n",
    "df = influencers_df[influencers_df['year']==x]\n",
    "for i in range(8):\n",
    "    for ii, row in df.iterrows():\n",
    "        if row['handler'] in handler_counts_each_topic[i]:\n",
    "            count = handler_counts_each_topic[i][row['handler']]\n",
    "            table.append([i, 9-ii+(x-2014)*10, count, topic_names[i], row['name']])\n",
    "            \n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(table[1:], columns=table[0]).to_csv('output/topic_influencer/2016.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most dominant topics and number of documents for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "series_topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "idx = []\n",
    "topic_counts = []\n",
    "keywords = []\n",
    "topic_contributions = []\n",
    "\n",
    "for (i, count) in series_topic_counts.iteritems():\n",
    "    idx.append(int(i))\n",
    "    keywords.append(\", \".join([word for word, _ in optimal_model.show_topic(int(i))]))\n",
    "    topic_counts.append(count)\n",
    "    topic_contributions.append(str(round(100*count/series_topic_counts.sum(), 2))+'%')\n",
    "    \n",
    "df_dominant_topics = pd.DataFrame.from_dict({\n",
    "    'Dominant_Topic_Num': idx,\n",
    "    'Topic_Keywords': keywords, \n",
    "    'Num_Documents': topic_counts, \n",
    "    'Perc_Documents': topic_contributions\n",
    "})\n",
    "\n",
    "# Show\n",
    "pd.options.display.max_colwidth = 100\n",
    "df_dominant_topics.set_index('Dominant_Topic_Num', inplace=True)\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "\n",
    "writer = ExcelWriter('output/2014/data/2014_13_clusters.xlsx')\n",
    "new_df = df_dominant_topics.copy()\n",
    "new_df.index += 1\n",
    "new_df.to_excel(writer,'topic_rank')\n",
    "\n",
    "for topic_no in range(13):\n",
    "    new_df = sent_topics_sorteddf_mallet.iloc[topic_no*40:((topic_no+1)*40),[0,4]]\n",
    "    new_df.set_index('Document_No', inplace=True)\n",
    "    \n",
    "    replied_texts = []\n",
    "    for i, row in new_df.iterrows():\n",
    "        if replied_ids[indices[i]]:\n",
    "            try:\n",
    "                text = api.get_status(replied_ids[indices[i]], tweet_mode='extended').full_text\n",
    "                replied_texts.append(text)\n",
    "            except:\n",
    "                replied_texts.append('')\n",
    "        else:\n",
    "            replied_texts.append('')\n",
    "            \n",
    "    new_df['In_Reply_To'] = replied_texts\n",
    "    \n",
    "    new_df.reset_index(inplace=True)\n",
    "    new_df.drop(['Document_No'], axis=1, inplace=True)\n",
    "    new_df.to_excel(writer,'topic_'+str(topic_no+1))\n",
    "    \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model_list[0])\n",
    "vis = pyLDAvis.gensim.prepare(model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'output/2014/visually_aided_clusters/2014_13_clusters.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [strip_punc(w) for w in stopwords.words('english')]\n",
    "stop_words.extend([\n",
    "    'i',\n",
    "    'u',\n",
    "    'r',\n",
    "    'im',\n",
    "    'cant',\n",
    "    'would',\n",
    "    'family',\n",
    "    'domestic',\n",
    "    'violence',\n",
    "    'australia',\n",
    "    'australian',\n",
    "    'dv',\n",
    "    'fv',\n",
    "    'au',\n",
    "    'via',\n",
    "    'today',\n",
    "    'thing',\n",
    "    'make',\n",
    "    'talk',\n",
    "    'due',\n",
    "    'day',\n",
    "    'month',\n",
    "    'find',\n",
    "    'show',\n",
    "    'put',\n",
    "    'part',\n",
    "    'time',\n",
    "    'yeah',\n",
    "    'deal',\n",
    "    'big',\n",
    "    'level',\n",
    "    'focus',\n",
    "    'theyre',\n",
    "    'list',\n",
    "    'top',\n",
    "    'give',\n",
    "    'situation',\n",
    "    'lot',\n",
    "    'hold',\n",
    "    'number',\n",
    "    'include',\n",
    "    'form',\n",
    "    'back',\n",
    "    'involve',\n",
    "    'link',\n",
    "    'real',\n",
    "    'get',\n",
    "    'go',\n",
    "    'have',\n",
    "    'do',\n",
    "    'take',\n",
    "    'time','year','month','week','day','say'\n",
    "])\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = ''\n",
    "    for token in text.split():\n",
    "        \n",
    "        # Cleaning\n",
    "        if token[0] in ['@','$','%','^','&','*'] or token.startswith('http'):\n",
    "            continue\n",
    "\n",
    "        # Remove puctuations, lower case\n",
    "        token = strip_punc(token.lower())\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemma = lemmatize(token)\n",
    "\n",
    "        if lemma and lemma not in stop_words:\n",
    "            cleaned_text += lemma + ' '\n",
    "    \n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "raw_docs = []\n",
    "conn = sqlite3.connect('dpc.db')\n",
    "\n",
    "df = pd.read_sql('SELECT text, [extended_tweet.full_text] FROM tweets', conn)\n",
    "conn.close()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    text = ''\n",
    "    if row['extended_tweet.full_text']:\n",
    "        text = clean_text(row['extended_tweet.full_text'])\n",
    "        raw_docs.append(row['extended_tweet.full_text'])\n",
    "    else:\n",
    "        text = clean_text(row['text'])\n",
    "        raw_docs.append(row['text'])\n",
    "    docs.append(text)\n",
    "\n",
    "print(len(docs), docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "vector_space = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victim_docs = []\n",
    "victim_raw_docs = []\n",
    "for text in sent_topics_sorteddf_mallet[(sent_topics_sorteddf_mallet['Dominant_Topic']==2.0) | (sent_topics_sorteddf_mallet['Dominant_Topic']==5.0)]['Text']:\n",
    "    victim_docs.append(clean_text(text))\n",
    "    victim_raw_docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.transform(victim_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for vector in vectors:\n",
    "    scores.append(np.sum(vector))\n",
    "scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(scores)\n",
    "indices = (-arr).argsort()[:40]\n",
    "print(scores[indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in indices:\n",
    "    print(victim_raw_docs[idx], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ''\n",
    "for text in sent_topics_sorteddf_mallet[(sent_topics_sorteddf_mallet['Dominant_Topic']==2.0) | (sent_topics_sorteddf_mallet['Dominant_Topic']==5.0)]['Text']:\n",
    "    doc += clean_text(text) + ' '\n",
    "doc = doc.strip()    \n",
    "\n",
    "df = pd.DataFrame(vectorizer.transform([doc]).T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "print(df.sort_values(by=[\"tfidf\"],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "x = 0\n",
    "for text in victim_docs:\n",
    "    tokens = text.split()\n",
    "    for i in range(len(tokens)-1):\n",
    "        scores[(tokens[i], tokens[i+1])] = scores.get((tokens[i], tokens[i+1]),0)+1\n",
    "\n",
    "#     x += 1\n",
    "#     if x % 100 == 0:\n",
    "#         print(x, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in list(sorted(scores, key=scores.get, reverse=True))[:40]:\n",
    "    print(str(w)+':', scores[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(victim_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('dpc.db')\n",
    "\n",
    "df = pd.read_sql('SELECT count(*) FROM tweets where created_at like \"%2017\"', conn)\n",
    "conn.close()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
